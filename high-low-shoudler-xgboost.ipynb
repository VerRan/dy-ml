{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 瑜伽高低肩识别示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker==2.16.1\n",
      "  Using cached sagemaker-2.16.1.tar.gz (306 kB)\n",
      "Requirement already satisfied: boto3>=1.14.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==2.16.1) (1.17.27)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==2.16.1) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==2.16.1) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==2.16.1) (3.15.2)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==2.16.1) (0.1.5)\n",
      "Collecting smdebug-rulesconfig==0.1.5\n",
      "  Using cached smdebug_rulesconfig-0.1.5-py2.py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==2.16.1) (3.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==2.16.1) (20.9)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==2.16.1) (1.20.27)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==2.16.1) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==2.16.1) (0.3.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.27->boto3>=1.14.12->sagemaker==2.16.1) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.27->boto3>=1.14.12->sagemaker==2.16.1) (1.26.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==2.16.1) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==2.16.1) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging>=20.0->sagemaker==2.16.1) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker==2.16.1) (1.15.0)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.16.1-py2.py3-none-any.whl size=435538 sha256=a91eaefe7c3a799d8c33e258c1d4f438ba302a4ed6fe1c63187107afef629fc7\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/e4/18/de/1f047858249ced343e3ebd5a93847e05c7a027363e252f0d87\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: smdebug-rulesconfig, sagemaker\n",
      "\u001b[33m  WARNING: The script sagemaker-upgrade-v2 is installed in '/home/ec2-user/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed sagemaker-2.16.1 smdebug-rulesconfig-0.1.5\n"
     ]
    }
   ],
   "source": [
    "! pip install sagemaker==2.16.1 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "### 参数设置\n",
    "下面参数设置将在后续使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket for saving processing job outputs\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = \"/home/sagemaker-user/dy-ml/data_wrangler_flows\"\n",
    "flow_id = f\"{time.strftime('%d-%H-%M-%S', time.gmtime())}-{str(uuid.uuid4())[:8]}\"\n",
    "flow_name = f\"flow-{flow_id}\"\n",
    "flow_uri = f\"s3://{bucket}/{prefix}/{flow_name}.flow\"\n",
    "\n",
    "flow_file_name = \"dy-data-processing.flow\"\n",
    "\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "container_uri = \"174368400705.dkr.ecr.us-west-2.amazonaws.com/sagemaker-data-wrangler-container:1.0.1\"\n",
    "\n",
    "# Processing Job Resources Configurations\n",
    "# Data wrangler processing job only supports 1 instance.\n",
    "instance_count = 1\n",
    "instance_type = \"ml.m5.4xlarge\"\n",
    "\n",
    "# Processing Job Path URI Information\n",
    "output_prefix = f\"export-{flow_name}/output\"\n",
    "output_path = f\"s3://{bucket}/{output_prefix}\"\n",
    "output_name = \"21a6c8d8-e540-4d1a-ad10-7803bc6e7c66.default\"\n",
    "\n",
    "processing_job_name = f\"data-wrangler-flow-processing-{flow_id}\"\n",
    "\n",
    "processing_dir = \"/opt/ml/processing\"\n",
    "\n",
    "# Modify the variable below to specify the content type to be used for writing each output\n",
    "# Currently supported options are 'CSV' or 'PARQUET', and default to 'CSV'\n",
    "output_content_type = \"CSV\"\n",
    "\n",
    "# URL to use for sagemaker client.\n",
    "# If this is None, boto will automatically construct the appropriate URL to use\n",
    "# when communicating with sagemaker.\n",
    "sagemaker_endpoint_url = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推送数据处理流程文件到S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Wrangler Flow notebook uploaded to s3://sagemaker-us-west-2-517141035927//home/sagemaker-user/dy-ml/data_wrangler_flows/flow-22-02-50-44-bcd068dd.flow\n"
     ]
    }
   ],
   "source": [
    "# Load .flow file\n",
    "with open(flow_file_name) as f:\n",
    "    flow = json.load(f)\n",
    "\n",
    "# Upload to S3\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(flow_file_name, bucket, f\"{prefix}/{flow_name}.flow\")\n",
    "\n",
    "print(f\"Data Wrangler Flow notebook uploaded to {flow_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建数据处理流程参数 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flow_notebook_processing_input(base_dir, flow_s3_uri):\n",
    "    return {\n",
    "        \"InputName\": \"flow\",\n",
    "        \"S3Input\": {\n",
    "            \"LocalPath\": f\"{base_dir}/flow\",\n",
    "            \"S3Uri\": flow_s3_uri,\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3InputMode\": \"File\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "def create_s3_processing_input(base_dir, name, dataset_definition):\n",
    "    return {\n",
    "        \"InputName\": name,\n",
    "        \"S3Input\": {\n",
    "            \"LocalPath\": f\"{base_dir}/{name}\",\n",
    "            \"S3Uri\": dataset_definition[\"s3ExecutionContext\"][\"s3Uri\"],\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3InputMode\": \"File\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "def create_redshift_processing_input(base_dir, name, dataset_definition):\n",
    "    return {\n",
    "        \"InputName\": name,\n",
    "        \"DatasetDefinition\": {\n",
    "            \"RedshiftDatasetDefinition\": {\n",
    "                \"ClusterId\": dataset_definition[\"clusterIdentifier\"],\n",
    "                \"Database\": dataset_definition[\"database\"],\n",
    "                \"DbUser\": dataset_definition[\"dbUser\"],\n",
    "                \"QueryString\": dataset_definition[\"queryString\"],\n",
    "                \"ClusterRoleArn\": dataset_definition[\"unloadIamRole\"],\n",
    "                \"OutputS3Uri\": f'{dataset_definition[\"s3OutputLocation\"]}{name}/',\n",
    "                \"OutputFormat\": dataset_definition[\"outputFormat\"].upper(),\n",
    "            },\n",
    "            \"LocalPath\": f\"{base_dir}/{name}\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "def create_athena_processing_input(base_dir, name, dataset_definition):\n",
    "    return {\n",
    "        \"InputName\": name,\n",
    "        \"DatasetDefinition\": {\n",
    "            \"AthenaDatasetDefinition\": {\n",
    "                \"Catalog\": dataset_definition[\"catalogName\"],\n",
    "                \"Database\": dataset_definition[\"databaseName\"],\n",
    "                \"QueryString\": dataset_definition[\"queryString\"],\n",
    "                \"OutputS3Uri\": f'{dataset_definition[\"s3OutputLocation\"]}{name}/',\n",
    "                \"OutputFormat\": dataset_definition[\"outputFormat\"].upper(),\n",
    "            },\n",
    "            \"LocalPath\": f\"{base_dir}/{name}\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "def create_processing_inputs(processing_dir, flow, flow_uri):\n",
    "    \"\"\"Helper function for creating processing inputs\n",
    "    :param flow: loaded data wrangler flow notebook\n",
    "    :param flow_uri: S3 URI of the data wrangler flow notebook\n",
    "    \"\"\"\n",
    "    processing_inputs = []\n",
    "    flow_processing_input = create_flow_notebook_processing_input(processing_dir, flow_uri)\n",
    "    processing_inputs.append(flow_processing_input)\n",
    "\n",
    "    for node in flow[\"nodes\"]:\n",
    "        if \"dataset_definition\" in node[\"parameters\"]:\n",
    "            data_def = node[\"parameters\"][\"dataset_definition\"]\n",
    "            name = data_def[\"name\"]\n",
    "            source_type = data_def[\"datasetSourceType\"]\n",
    "\n",
    "            if source_type == \"S3\":\n",
    "                s3_processing_input = create_s3_processing_input(\n",
    "                    processing_dir, name, data_def)\n",
    "                processing_inputs.append(s3_processing_input)\n",
    "            elif source_type == \"Athena\":\n",
    "                athena_processing_input = create_athena_processing_input(\n",
    "                    processing_dir, name, data_def)\n",
    "                processing_inputs.append(athena_processing_input)\n",
    "            elif source_type == \"Redshift\":\n",
    "                redshift_processing_input = create_redshift_processing_input(\n",
    "                    processing_dir, name, data_def)\n",
    "                processing_inputs.append(redshift_processing_input)\n",
    "            else:\n",
    "                raise ValueError(f\"{source_type} is not supported for Data Wrangler Processing.\")\n",
    "    return processing_inputs\n",
    "\n",
    "def create_container_arguments(output_name, output_content_type):\n",
    "    output_config = {\n",
    "        output_name: {\n",
    "            \"content_type\": output_content_type\n",
    "        }\n",
    "    }\n",
    "    return [f\"--output-config '{json.dumps(output_config)}'\"]\n",
    "\n",
    "# Create Processing Job Arguments\n",
    "processing_job_arguments = {\n",
    "    \"AppSpecification\": {\n",
    "        \"ContainerArguments\": create_container_arguments(output_name, output_content_type),\n",
    "        \"ImageUri\": container_uri,\n",
    "    },\n",
    "    \"ProcessingInputs\": create_processing_inputs(processing_dir, flow, flow_uri),\n",
    "    \"ProcessingOutputConfig\": {\n",
    "        \"Outputs\": [\n",
    "            {\n",
    "                \"OutputName\": output_name,\n",
    "                \"S3Output\": {\n",
    "                    \"S3Uri\": output_path,\n",
    "                    \"LocalPath\": os.path.join(processing_dir, \"output\"),\n",
    "                    \"S3UploadMode\": \"EndOfJob\",\n",
    "                }\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    \"ProcessingJobName\": processing_job_name,\n",
    "    \"ProcessingResources\": {\n",
    "        \"ClusterConfig\": {\n",
    "            \"InstanceCount\": instance_count,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"VolumeSizeInGB\": 30,\n",
    "        }\n",
    "    },\n",
    "    \"RoleArn\": iam_role,\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 86400,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始执行预处理任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "Completed\n",
      "{'ProcessingInputs': [{'InputName': 'flow', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-517141035927//home/sagemaker-user/dy-ml/data_wrangler_flows/flow-12-13-32-50-b38e244f.flow', 'LocalPath': '/opt/ml/processing/flow', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated'}}, {'InputName': 'high-low-shoulder-data.csv', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://aws-glue-517141035927-us-west-2/high-low-shoulder-data.csv', 'LocalPath': '/opt/ml/processing/high-low-shoulder-data.csv', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': '21a6c8d8-e540-4d1a-ad10-7803bc6e7c66.default', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-517141035927/export-flow-12-13-32-50-b38e244f/output', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}, 'AppManaged': False}]}, 'ProcessingJobName': 'data-wrangler-flow-processing-12-13-32-50-b38e244f', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.4xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}, 'AppSpecification': {'ImageUri': '174368400705.dkr.ecr.us-west-2.amazonaws.com/sagemaker-data-wrangler-container:1.0.1', 'ContainerArguments': ['--output-config \\'{\"21a6c8d8-e540-4d1a-ad10-7803bc6e7c66.default\": {\"content_type\": \"CSV\"}}\\'']}, 'RoleArn': 'arn:aws:iam::517141035927:role/service-role/AmazonSageMaker-ExecutionRole-20190819T155749', 'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:517141035927:processing-job/data-wrangler-flow-processing-12-13-32-50-b38e244f', 'ProcessingJobStatus': 'Completed', 'ProcessingEndTime': datetime.datetime(2021, 3, 12, 13, 37, 29, 673000, tzinfo=tzlocal()), 'ProcessingStartTime': datetime.datetime(2021, 3, 12, 13, 36, 45, 821000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2021, 3, 12, 13, 37, 29, 676000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2021, 3, 12, 13, 32, 55, 141000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '1060c53f-87b8-4f28-9277-575c044df5ca', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '1060c53f-87b8-4f28-9277-575c044df5ca', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1951', 'date': 'Fri, 12 Mar 2021 13:37:55 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "sagemaker_client = boto3.client(\"sagemaker\", endpoint_url=sagemaker_endpoint_url)\n",
    "create_response = sagemaker_client.create_processing_job(**processing_job_arguments)\n",
    "\n",
    "status = sagemaker_client.describe_processing_job(ProcessingJobName=processing_job_name)\n",
    "\n",
    "while status[\"ProcessingJobStatus\"] == \"InProgress\":\n",
    "    status = sagemaker_client.describe_processing_job(ProcessingJobName=processing_job_name)\n",
    "    print(status[\"ProcessingJobStatus\"])\n",
    "    time.sleep(60)\n",
    "\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看预处理结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export-flow-12-13-32-50-b38e244f/output/data-wrangler-flow-processing-12-13-32-50-b38e244f/21a6c8d8-e540-4d1a-ad10-7803bc6e7c66/default/part-00000-5940954b-30c5-43fb-9588-ae766648b34e-c000.csv\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "list_response = s3_client.list_objects_v2(Bucket=bucket, Prefix=output_prefix)\n",
    "\n",
    "training_path = None\n",
    "\n",
    "for content in list_response[\"Contents\"]:\n",
    "    if \"_SUCCESS\" not in content[\"Key\"]:\n",
    "        training_path = content[\"Key\"]\n",
    "\n",
    "print(training_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "### 定义算法&超参数&训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n",
    "hyperparameters = {\n",
    "    \"max_depth\":\"5\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": \"4\",\n",
    "    \"eval_metric\": \"auc\"\n",
    "}\n",
    "train_content_type = (\n",
    "    \"application/x-parquet\" if output_content_type.upper() == \"PARQUET\"\n",
    "    else \"text/csv\"\n",
    ")\n",
    "train_input = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=f\"s3://{bucket}/{training_path}\",\n",
    "    content_type=train_content_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过评估器触发训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-17 11:32:52 Starting - Starting the training job...\n",
      "2021-03-17 11:32:54 Starting - Launching requested ML instances.........\n",
      "2021-03-17 11:34:29 Starting - Preparing the instances for training...\n",
      "2021-03-17 11:35:05 Downloading - Downloading input data...\n",
      "2021-03-17 11:35:50 Training - Training image download completed. Training in progress..\u001b[34m[2021-03-17 11:35:52.339 ip-10-0-162-206.us-west-2.compute.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter eval_metric value auc to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 476 rows\u001b[0m\n",
      "\u001b[34m[0]#011train-auc:0.69690\u001b[0m\n",
      "\u001b[34m[1]#011train-auc:0.72698\u001b[0m\n",
      "\u001b[34m[2]#011train-auc:0.74842\u001b[0m\n",
      "\u001b[34m[3]#011train-auc:0.81074\u001b[0m\n",
      "\n",
      "2021-03-17 11:36:02 Uploading - Uploading generated training model\n",
      "2021-03-17 11:36:02 Completed - Training job completed\n",
      "Training seconds: 57\n",
      "Billable seconds: 57\n"
     ]
    }
   ],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    iam_role,\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    ")\n",
    "estimator.fit({\"train\": train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 部署模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "xgb_predictor = estimator.deploy(\n",
    "    initial_instance_count = 1, \n",
    "    instance_type = 'ml.m4.xlarge',\n",
    "    serializer=CSVSerializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = xgb_predictor.predict(input).decode('utf-8')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用sagemaker endpoint推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "test_data = pd.read_csv('./test.csv')\n",
    "test_data=test_data.drop(test_data.columns[0],axis=1)##删除没有列名的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000128</td>\n",
       "      <td>6.999593e-09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022318</td>\n",
       "      <td>1.336634e-03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.062001</td>\n",
       "      <td>6.692099e-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.021154</td>\n",
       "      <td>2.386069e-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009066</td>\n",
       "      <td>9.534239e-03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.009085</td>\n",
       "      <td>3.998876e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>0.016884</td>\n",
       "      <td>1.601127e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>0.011053</td>\n",
       "      <td>1.626371e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>0.017410</td>\n",
       "      <td>1.308095e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.012307</td>\n",
       "      <td>1.199654e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>476 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0             1  2\n",
       "0    0.000128  6.999593e-09  1\n",
       "1    0.022318  1.336634e-03  1\n",
       "2    0.062001  6.692099e-02  1\n",
       "3    0.021154  2.386069e-02  1\n",
       "4    0.009066  9.534239e-03  1\n",
       "..        ...           ... ..\n",
       "471  0.009085  3.998876e-03  0\n",
       "472  0.016884  1.601127e-02  0\n",
       "473  0.011053  1.626371e-02  0\n",
       "474  0.017410  1.308095e-02  0\n",
       "475  0.012307  1.199654e-02  0\n",
       "\n",
       "[476 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用predictor 推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推理单条记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6214224696159363\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "endpoint_name = \"sagemaker-xgboost-2021-03-17-11-18-11-571\" \n",
    "input = '0.009085,1.601127e-02'\n",
    "\n",
    "xgb_predictor = sagemaker.Predictor(endpoint_name=endpoint_name,\n",
    "                                   sagemaker_session=sess,\n",
    "                                   serializer=CSVSerializer())\n",
    "response = xgb_predictor.predict(input).decode('utf-8')\n",
    "print(response)\n",
    "if float(response)>0.652 :\n",
    "    print(\"1\")\n",
    "else :\n",
    "    print(\"0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推理训练数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "for i in range(0, len(test_data)):\n",
    "    input = str(test_data.iloc[i]['0'])+\",\"+str(test_data.iloc[i]['1'])\n",
    "    response = xgb_predictor.predict(input).decode('utf-8')\n",
    "    y_true.append(round(test_data.iloc[i]['2']))\n",
    "    if float(response)>0.6 :\n",
    "        response=1\n",
    "    else :\n",
    "        response=0\n",
    "    y_pred.append(response)\n",
    "#     print(test_data.iloc[i]['0'], test_data.iloc[i]['1'], test_data.iloc[i]['2'],response)\n",
    "#     print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[124  34]\n",
      " [108 210]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, f1_score, precision_score, recall_score\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TP =124 ，预测正确 存在高低肩问题，而且实际有高低肩问题的数量\n",
    "* FN =108 ，预测错误 不存在高低肩问题，而实际是有高低肩问题的数量\n",
    "* FP = 34 ，预测错误 存在高低肩问题，而实际是不存在高低肩问题的数量\n",
    "* TN = 201,预测正确 不存在高低肩问题，实际也是不存在高低肩问题的数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准确率\n",
    "Accuracy = TP+TN/TP+TN+FP+FN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.7016806722689075\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Accuracy = (124+210)/(124+34+108+210)\n",
    "print('Accuracy is ',Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 精度\n",
    "预测存在高低肩问题 ：在所有类别中，我们正确预测了多少\n",
    "precision = TP/（TP+FP）\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision= 120/(120+34)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.860655737704918\n"
     ]
    }
   ],
   "source": [
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "print('precision:', precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 召回\n",
    "在所有为正的类别中，我们正确预测了多少。\n",
    "recall = tp/(tp+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.660377358490566\n"
     ]
    }
   ],
   "source": [
    "recall = recall_score(y_true, y_pred)\n",
    "print('recall:', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1得分\n",
    "F1 = 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7473309608540925"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
